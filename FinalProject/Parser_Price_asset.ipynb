{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76487338-098d-4062-9d92-1cbf11448e4c",
   "metadata": {},
   "source": [
    "Файл повторяет парсинг позиций в точности до $\\pm$10 строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2a7ffd-1029-4b68-8bc4-2cc6ae12b796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:52.983627Z",
     "iopub.status.busy": "2023-12-03T17:50:52.983264Z",
     "iopub.status.idle": "2023-12-03T17:50:53.390285Z",
     "shell.execute_reply": "2023-12-03T17:50:53.389750Z",
     "shell.execute_reply.started": "2023-12-03T17:50:52.983600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83350ebb-d6cc-4fbc-b234-5941ba647d73",
   "metadata": {},
   "source": [
    "# Демонстрация того, как будет происходить процесс совмещения на двух днях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9412618-5e19-4454-a0cf-e22c0d95ad24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.392266Z",
     "iopub.status.busy": "2023-12-03T17:50:53.391317Z",
     "iopub.status.idle": "2023-12-03T17:50:53.395346Z",
     "shell.execute_reply": "2023-12-03T17:50:53.394598Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.392230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ссылка на данные\n",
    "url1 = \"https://iss.moex.com/iss/history/engines/stock/markets/shares/sessions/2/securities/LKOH.json?from=2023-11-07&till=2023-11-07&table_type=full\"\n",
    "url2 = \"https://iss.moex.com/iss/history/engines/stock/markets/shares/sessions/2/securities/LKOH.json?from=2023-11-08&till=2023-11-08&table_type=full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df49ced-8d39-4ebf-9824-9a875770d548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.397764Z",
     "iopub.status.busy": "2023-12-03T17:50:53.397121Z",
     "iopub.status.idle": "2023-12-03T17:50:53.639648Z",
     "shell.execute_reply": "2023-12-03T17:50:53.638865Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.397717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Отправляем GET-запрос по указанной ссылке\n",
    "response = rq.get(url1)\n",
    "\n",
    "# Проверяем статус ответа\n",
    "if response.status_code == 200:\n",
    "    # Загружаем данные из ответа в переменную data в случае успеха\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Ошибка при получении данных\")\n",
    "    \n",
    "\n",
    "columns = response.json()[\"history\"]['columns']\n",
    "\n",
    "data = response.json()[\"history\"][\"data\"]\n",
    "\n",
    "df1 = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "\n",
    "# Отправляем GET-запрос по указанной ссылке\n",
    "response = rq.get(url2)\n",
    "\n",
    "# Проверяем статус ответа\n",
    "if response.status_code == 200:\n",
    "    # Загружаем данные из ответа в переменную data в случае успеха\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Ошибка при получении данных\")\n",
    "\n",
    "\n",
    "columns = response.json()[\"history\"]['columns']\n",
    "\n",
    "data = response.json()[\"history\"][\"data\"]\n",
    "\n",
    "df2 = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984879f5-fc18-4cb3-a5a7-17aec9331ec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.640881Z",
     "iopub.status.busy": "2023-12-03T17:50:53.640572Z",
     "iopub.status.idle": "2023-12-03T17:50:53.656875Z",
     "shell.execute_reply": "2023-12-03T17:50:53.655895Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.640854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TRADEDATE SECID                   BOARDNAME BOARDID SHORTNAME  \\\n",
      "0  2023-11-07  LKOH  Т+: Акции и ДР - безадрес.    TQBR    ЛУКОЙЛ   \n",
      "\n",
      "      REGNUMBER          ISIN LISTNAME  FACEVALUE CURRENCYID  ...  \\\n",
      "0  1-01-00077-A  RU0009024277        1      0.025        SUR  ...   \n",
      "\n",
      "  MARKETPRICE3  MARKETPRICE3TRADESVALUE DECIMALS          TYPE  \\\n",
      "0         None                        0        1  common_share   \n",
      "\n",
      "   CLOSEAUCTIONPRICE  WAVAL  LASTPRICE  MARKETPRICE3TRADESVALUECUR  \\\n",
      "0                  0      0     7333.5                        None   \n",
      "\n",
      "  MARKETPRICE3CUR  TRADINGSESSION  \n",
      "0            None               2  \n",
      "\n",
      "[1 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1.sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46119e7-10c9-4258-9a9a-4256d841722d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.658407Z",
     "iopub.status.busy": "2023-12-03T17:50:53.658160Z",
     "iopub.status.idle": "2023-12-03T17:50:53.681193Z",
     "shell.execute_reply": "2023-12-03T17:50:53.680174Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.658386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TRADEDATE SECID                   BOARDNAME BOARDID SHORTNAME  \\\n",
      "0  2023-11-07  LKOH  Т+: Акции и ДР - безадрес.    TQBR    ЛУКОЙЛ   \n",
      "1  2023-11-08  LKOH  Т+: Акции и ДР - безадрес.    TQBR    ЛУКОЙЛ   \n",
      "\n",
      "      REGNUMBER          ISIN LISTNAME  FACEVALUE CURRENCYID  ...  \\\n",
      "0  1-01-00077-A  RU0009024277        1      0.025        SUR  ...   \n",
      "1  1-01-00077-A  RU0009024277        1      0.025        SUR  ...   \n",
      "\n",
      "  MARKETPRICE3  MARKETPRICE3TRADESVALUE DECIMALS          TYPE  \\\n",
      "0         None                        0        1  common_share   \n",
      "1         None                        0        1  common_share   \n",
      "\n",
      "   CLOSEAUCTIONPRICE  WAVAL  LASTPRICE  MARKETPRICE3TRADESVALUECUR  \\\n",
      "0                  0      0     7333.5                        None   \n",
      "1                  0      0     7295.0                        None   \n",
      "\n",
      "  MARKETPRICE3CUR  TRADINGSESSION  \n",
      "0            None               2  \n",
      "1            None               2  \n",
      "\n",
      "[2 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "frames = [df1, df2]\n",
    "\n",
    "result = pd.concat(frames).sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1)\n",
    "filename = \"aboba_date.csv\"\n",
    "result.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd96785-8d91-40b0-939d-b2179b04869c",
   "metadata": {},
   "source": [
    "Далее будет пробегать по массиву дат и таким образом сформируем всю таблицу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8510040-967f-4be1-9e00-8a6631aa510d",
   "metadata": {},
   "source": [
    "Ниже я задаю массив всех дат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8fe89-0798-4fae-aa55-ce5d28dec98c",
   "metadata": {},
   "source": [
    "# Парсинг данных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4b4d0-c4a4-440e-af0f-d9fb3bcaceea",
   "metadata": {},
   "source": [
    "Сначала мы создаём массив, который содержить все даты, за которые мы хотим получить данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0d48f5-16bd-4ba9-9b74-5675afafd480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.683109Z",
     "iopub.status.busy": "2023-12-03T17:50:53.682318Z",
     "iopub.status.idle": "2023-12-03T17:50:53.691528Z",
     "shell.execute_reply": "2023-12-03T17:50:53.690455Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.683059Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = datetime(2020, 5, 8)\n",
    "end_date = datetime.now()\n",
    "\n",
    "current_date = start_date\n",
    "array_date = []\n",
    "while current_date <= end_date:\n",
    "    array_date += [current_date.strftime(\"%Y-%m-%d\")]\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d4d37-ee62-457a-80e0-e39f13a36002",
   "metadata": {},
   "source": [
    "Тут мы формируем массив из дней, а затем, если сервер работает хорошо, то возвращаем данные за весь промежуток от 5 августа 2020 до сегодняшнего дня. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f9112-9f36-4685-9a4c-d9a69e343b23",
   "metadata": {},
   "source": [
    "Если сервер перестаёт отвечать, тогда полученные данные мы собираем в файл и при повторном запуске программа проверить наши файлы и начнёт с последнего записанного дня."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c28a5-2bf3-448d-8344-ca2987603012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T19:42:06.545174Z",
     "iopub.status.busy": "2023-12-01T19:42:06.544183Z",
     "iopub.status.idle": "2023-12-01T19:42:06.567181Z",
     "shell.execute_reply": "2023-12-01T19:42:06.566182Z",
     "shell.execute_reply.started": "2023-12-01T19:42:06.545174Z"
    }
   },
   "source": [
    "В качестве отладки печатается дата дня, который мы сейчас получаем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84838e65-1b77-4d33-b6a5-afbd54555f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.693797Z",
     "iopub.status.busy": "2023-12-03T17:50:53.692974Z",
     "iopub.status.idle": "2023-12-03T17:50:53.713984Z",
     "shell.execute_reply": "2023-12-03T17:50:53.713079Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.693751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл 'SBER_full_date_price.csv' существует.\n",
      "Файл 'GAZP_full_date_price.csv' существует.\n",
      "Файл 'LKOH_full_date_price.csv' существует.\n",
      "Файл 'ROSN_full_date_price.csv' существует.\n",
      "Файл 'MGNT_full_date_price.csv' существует.\n",
      "Файл 'VTBR_full_date_price.csv' существует.\n",
      "Файл 'SNGS_full_date_price.csv' существует.\n",
      "Файл 'YNDX_full_date_price.csv' существует.\n",
      "Файл 'AFLT_full_date_price.csv' существует.\n",
      "Файл 'ALRS_full_date_price.csv' существует.\n",
      "Файл 'TATN_full_date_price.csv' существует.\n",
      "Файл 'HYDR_full_date_price.csv' существует.\n",
      "Файл 'NLMK_full_date_price.csv' существует.\n",
      "Файл 'MOEX_full_date_price.csv' существует.\n",
      "Файл 'GMKN_full_date_price.csv' существует.\n",
      "Файл 'FIVE_full_date_price.csv' существует.\n",
      "Файл 'MAGN_full_date_price.csv' существует.\n"
     ]
    }
   ],
   "source": [
    "def add_new_day(date_today: str, ticker_name: str): \n",
    "    url = f\"https://iss.moex.com/iss/history/engines/stock/markets/shares/sessions/2/securities/{ticker_name}.json?from={date_today}&till={date_today}&table_type=full\" \n",
    " \n",
    "    while True: \n",
    "        try: \n",
    "            # Ваш код, который может вызвать ошибку \n",
    "            response = rq.get(url) \n",
    "            # break  # Если ошибки не возникло, выходим из цикла \n",
    "        except Exception as e: \n",
    "            # Обработка других исключений \n",
    "            print(f\"Произошла ошибка: {e}\") \n",
    "        else: \n",
    "            break \n",
    " \n",
    "    # Проверяем статус ответа \n",
    "    if response.status_code != 200: \n",
    "        print(\"Ошибка при получении данных\") \n",
    "        add_new_day(date_today) \n",
    " \n",
    "    df_for_add = pd.DataFrame(response.json()[\"history\"][\"data\"], columns=response.json()[\"history\"]['columns']) \n",
    "    df_for_add.sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1) \n",
    "    \n",
    " \n",
    "    return df_for_add \n",
    " \n",
    "def process_ticker(ticker_name): \n",
    "    if (os.path.exists(f\"{ticker_name}_full_date_price.csv\") or os.path.exists(f\"{ticker_dict[ticker_name]}_full_date_price.csv\")):\n",
    "        print(f\"Файл '{ticker_name}_full_date_price.csv' существует.\") \n",
    "        df = pd.read_csv(f\"{ticker_name}_full_date_price.csv\") \n",
    "        last_valid_date_index = df['TRADEDATE'].last_valid_index() \n",
    "        last_valid_date_value  = df['TRADEDATE'][last_valid_date_index] \n",
    "        date_tomorrow = (datetime.strptime(last_valid_date_value, '%Y-%m-%d') + timedelta(days=1)).strftime(\"%Y-%m-%d\") \n",
    "        print(date_tomorrow) \n",
    "    # Вывод первых нескольких строк DataFrame для проверки \n",
    "        if(not date_tomorrow in array_date): \n",
    "            pass \n",
    "        else: \n",
    "            df_array = [df] \n",
    "            for date in array_date[array_date.index(date_tomorrow)+1:]: \n",
    "                df_array += [add_new_day(date, ticker_name)] \n",
    "                print(date) \n",
    "            df = pd.concat(df_array).sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1) \n",
    "            print(df) \n",
    "            filename = f\"{ticker_name}_full_date_price.csv\" \n",
    "            df.to_csv(filename, sep=',', index=False, encoding='utf-8') \n",
    "    else: \n",
    "        print(f\"Файл '{ticker_name}_full_date_price.csv' не существует.\") \n",
    "        df_array = [] \n",
    "        for date in array_date: \n",
    "            df_array += [add_new_day(date, ticker_name)] \n",
    "            print(date) \n",
    "        df = pd.concat(df_array).sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1) \n",
    "        print(df) \n",
    "        filename = f\"{ticker_name}_full_date_price.csv\" \n",
    "        df.to_csv(filename, sep=',', index=False, encoding='utf-8') \n",
    "        \n",
    "ticker_names = ['SBER', 'GAZP', 'LKOH', 'VTBR', 'ROSN', 'MGNT', 'AFLT', 'ALRS', 'SNGS', 'YNDX', 'TATN', 'NLMK', 'HYDR', 'MOEX', 'FIVE', 'GMKN', 'MAGN']\n",
    "short_ticker_names = ['sr', 'gz', 'lk', 'vb', 'rn', 'mn', 'af', 'al', 'sn', 'yn', 'tt', 'nm', 'hy', 'me', 'fv', 'gk', 'mg', 'ml']\n",
    "\n",
    "ticker_dict = dict(zip(ticker_names, short_ticker_names))\n",
    "\n",
    "with ThreadPoolExecutor() as executor: \n",
    "    executor.map(process_ticker, ticker_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bbca5-4239-40bd-a01a-97b44e01bab7",
   "metadata": {},
   "source": [
    "Удаление повторяющихся данных (в силу того, как МосБиржа возвращала данные (в выходные дни вместо пустого запроса она возвращает последнюю цену за последний рабочий день, то есть 14*20 одинаковых записей, в силу чего эта ячейка позволяла сэкономить, примерно, 300 МБ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12cc0098-74c0-4bbc-8a36-bf1ef9faf733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.716688Z",
     "iopub.status.busy": "2023-12-03T17:50:53.716255Z",
     "iopub.status.idle": "2023-12-03T17:50:53.724333Z",
     "shell.execute_reply": "2023-12-03T17:50:53.723000Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.716666Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    " \n",
    "def rename_files(ticker_names): \n",
    "    # Путь к папке с файлами \n",
    "    folder_path = '.' \n",
    " \n",
    "    # Получаем список файлов в папке \n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))] \n",
    " \n",
    "    for file_name in files: \n",
    "        # Проверяем, что файл имеет нужное расширение и соответствует шаблону \n",
    "        if file_name.endswith('.csv') and any(ticker in file_name for ticker in ticker_names): \n",
    "            # Извлекаем имя тикера из имени файла \n",
    "            for ticker, short_ticker in zip(ticker_names, ['sr', 'gz', 'lk', 'vb', 'rn', 'mn', 'af', 'al', 'sn', 'yn', 'tt', 'nm', 'hy', 'me', 'fv', 'gk', 'mg', 'ml']): \n",
    "                if ticker in file_name: \n",
    "                    # Формируем новое имя файла \n",
    "                    new_file_name = file_name.replace(f\"{ticker}_full_date_price\", f\"{short_ticker}_full_date_price\") \n",
    "                    # Составляем полные пути к старому и новому файлу \n",
    "                    old_file_path = os.path.join(folder_path, file_name) \n",
    "                    new_file_path = os.path.join(folder_path, new_file_name) \n",
    "                    # Переименовываем файл \n",
    "                    os.rename(old_file_path, new_file_path) \n",
    "                    print(f'Файл {file_name} переименован в {new_file_name}') \n",
    "                    break \n",
    " \n",
    "rename_files(['SBER', 'GAZP', 'LKOH', 'VTBR', 'ROSN', 'MGNT', 'AFLT', 'ALRS', 'SNGS', 'YNDX', 'TATN', 'NLMK', 'HYDR', 'MOEX', 'FIVE', 'GMKN', 'MAGN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da8fc23-3c8d-409b-b49b-239032dca593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T17:50:53.725901Z",
     "iopub.status.busy": "2023-12-03T17:50:53.725595Z",
     "iopub.status.idle": "2023-12-03T17:50:54.414421Z",
     "shell.execute_reply": "2023-12-03T17:50:54.413757Z",
     "shell.execute_reply.started": "2023-12-03T17:50:53.725878Z"
    }
   },
   "outputs": [],
   "source": [
    "ticker_names=['sr', 'gz', 'lk', 'vb', 'rn', 'mn', 'af', 'al', 'sn', 'yn', 'tt', 'nm', 'hy', 'me', 'fv', 'gk', 'mg']\n",
    "\n",
    "for ticker_name in ticker_names:\n",
    "    your_file = f\"{ticker_name}_full_date_price.csv\" \n",
    "# Загрузка данных\n",
    "    df = pd.read_csv(your_file)\n",
    "\n",
    "# Удаление повторяющихся строк\n",
    "    unique_rows = df.drop_duplicates()\n",
    "\n",
    "# Сохранение результата\n",
    "    unique_rows.to_csv(your_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11151e32-8d0c-45cb-99bf-a203e5e5f0d9",
   "metadata": {},
   "source": [
    "Итоговые данные не идеальны, поэтому мы их будем обрабатывать в других данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
