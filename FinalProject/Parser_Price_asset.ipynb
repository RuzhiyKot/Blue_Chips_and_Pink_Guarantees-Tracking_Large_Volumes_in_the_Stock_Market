{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73180f25-3972-482c-8d51-e5559e98fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a7ffd-1029-4b68-8bc4-2cc6ae12b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rq\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83350ebb-d6cc-4fbc-b234-5941ba647d73",
   "metadata": {},
   "source": [
    "# Демонстрация того, как будет происходить процесс совмещения на двух днях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9412618-5e19-4454-a0cf-e22c0d95ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ссылка на данные\n",
    "url1 = \"https://iss.moex.com/iss/history/engines/stock/markets/shares/sessions/2/securities/LKOH.json?from=2023-11-07&till=2023-11-07&table_type=full\"\n",
    "url2 = \"https://iss.moex.com/iss/history/engines/stock/markets/shares/sessions/2/securities/LKOH.json?from=2023-11-08&till=2023-11-08&table_type=full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df49ced-8d39-4ebf-9824-9a875770d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отправляем GET-запрос по указанной ссылке\n",
    "response = rq.get(url1)\n",
    "\n",
    "# Проверяем статус ответа\n",
    "if response.status_code == 200:\n",
    "    # Загружаем данные из ответа в переменную data в случае успеха\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Ошибка при получении данных\")\n",
    "    \n",
    "\n",
    "columns = response.json()[\"history\"]['columns']\n",
    "\n",
    "data = response.json()[\"history\"][\"data\"]\n",
    "\n",
    "df1 = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "\n",
    "# Отправляем GET-запрос по указанной ссылке\n",
    "response = rq.get(url2)\n",
    "\n",
    "# Проверяем статус ответа\n",
    "if response.status_code == 200:\n",
    "    # Загружаем данные из ответа в переменную data в случае успеха\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(\"Ошибка при получении данных\")\n",
    "\n",
    "\n",
    "columns = response.json()[\"history\"]['columns']\n",
    "\n",
    "data = response.json()[\"history\"][\"data\"]\n",
    "\n",
    "df2 = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984879f5-fc18-4cb3-a5a7-17aec9331ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46119e7-10c9-4258-9a9a-4256d841722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2]\n",
    "\n",
    "result = pd.concat(frames).sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1)\n",
    "filename = \"test_date.csv\"\n",
    "result.to_csv(filename, sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd96785-8d91-40b0-939d-b2179b04869c",
   "metadata": {},
   "source": [
    "Далее будет пробегать по массиву дат и таким образом сформируем всю таблицу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8510040-967f-4be1-9e00-8a6631aa510d",
   "metadata": {},
   "source": [
    "Ниже я задаю массив всех дат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e8fe89-0798-4fae-aa55-ce5d28dec98c",
   "metadata": {},
   "source": [
    "# Парсинг данных цен"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4b4d0-c4a4-440e-af0f-d9fb3bcaceea",
   "metadata": {},
   "source": [
    "Сначала мы создаём массив, который содержить все даты, за которые мы хотим получить данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d48f5-16bd-4ba9-9b74-5675afafd480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = datetime(2020, 5, 8)\n",
    "end_date = datetime.now()\n",
    "\n",
    "current_date = start_date\n",
    "array_date = []\n",
    "while current_date <= end_date:\n",
    "    array_date += [current_date.strftime(\"%Y-%m-%d\")]\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d4d37-ee62-457a-80e0-e39f13a36002",
   "metadata": {},
   "source": [
    "Тут мы формируем массив из дней, а затем, если сервер работает хорошо, то возвращаем данные за весь промежуток от 5 августа 2020 до сегодняшнего дня. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f9112-9f36-4685-9a4c-d9a69e343b23",
   "metadata": {},
   "source": [
    "Если сервер перестаёт отвечать, тогда полученные данные мы собираем в файл и при повторном запуске программа проверить наши файлы и начнёт с последнего записанного дня."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c28a5-2bf3-448d-8344-ca2987603012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T19:42:06.545174Z",
     "iopub.status.busy": "2023-12-01T19:42:06.544183Z",
     "iopub.status.idle": "2023-12-01T19:42:06.567181Z",
     "shell.execute_reply": "2023-12-01T19:42:06.566182Z",
     "shell.execute_reply.started": "2023-12-01T19:42:06.545174Z"
    }
   },
   "source": [
    "В качестве отладки печатается дата дня, который мы сейчас получаем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84838e65-1b77-4d33-b6a5-afbd54555f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_new_day(date_today: str, ticker_name: str): \n",
    "    url = f\"https://iss.moex.com/iss/history/engines/stock/markets/shares/sessions/2/securities/{ticker_name}.json?from={date_today}&till={date_today}&table_type=full\" \n",
    " \n",
    "    while True: \n",
    "        try: \n",
    "            # Ваш код, который может вызвать ошибку \n",
    "            response = rq.get(url) \n",
    "            # break  # Если ошибки не возникло, выходим из цикла \n",
    "        except Exception as e: \n",
    "            # Обработка других исключений \n",
    "            print(f\"Произошла ошибка: {e}\") \n",
    "        else: \n",
    "            break \n",
    " \n",
    "    # Проверяем статус ответа \n",
    "    if response.status_code != 200: \n",
    "        print(\"Ошибка при получении данных\") \n",
    "        add_new_day(date_today) \n",
    " \n",
    "    df_for_add = pd.DataFrame(response.json()[\"history\"][\"data\"], columns=response.json()[\"history\"]['columns']) \n",
    "    df_for_add.sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1) \n",
    "    \n",
    " \n",
    "    return df_for_add \n",
    " \n",
    "def process_ticker(ticker_name): \n",
    "    if (Path(f\"{ticker_name}_full_date_price.csv\").exists() or Path(f\"{ticker_dict[ticker_name]}_full_date_price.csv\")).exists():\n",
    "        print(f\"Файл '{ticker_name}_full_date_price.csv' существует.\") \n",
    "        df = pd.read_csv(f\"{ticker_name}_full_date_price.csv\") \n",
    "        last_valid_date_index = df['TRADEDATE'].last_valid_index() \n",
    "        last_valid_date_value  = df['TRADEDATE'][last_valid_date_index] \n",
    "        date_tomorrow = (datetime.strptime(last_valid_date_value, '%Y-%m-%d') + timedelta(days=1)).strftime(\"%Y-%m-%d\") \n",
    "        print(date_tomorrow) \n",
    "    # Вывод первых нескольких строк DataFrame для проверки \n",
    "        if(not date_tomorrow in array_date): \n",
    "            pass \n",
    "        else: \n",
    "            df_array = [df] \n",
    "            for date in array_date[array_date.index(date_tomorrow)+1:]: \n",
    "                df_array += [add_new_day(date, ticker_name)] \n",
    "                print(date) \n",
    "            df = pd.concat(df_array).sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1) \n",
    "            print(df) \n",
    "            filename = f\"{ticker_name}_full_date_price.csv\" \n",
    "            df.to_csv(filename, sep=',', index=False, encoding='utf-8') \n",
    "    else: \n",
    "        print(f\"Файл '{ticker_name}_full_date_price.csv' не существует.\") \n",
    "        df_array = [] \n",
    "        for date in array_date: \n",
    "            df_array += [add_new_day(date, ticker_name)] \n",
    "            print(date) \n",
    "        df = pd.concat(df_array).sort_values(by=['TRADEDATE']).reset_index().drop('index', axis=1) \n",
    "        print(df) \n",
    "        filename = f\"{ticker_name}_full_date_price.csv\" \n",
    "        df.to_csv(filename, sep=',', index=False, encoding='utf-8') \n",
    "        \n",
    "ticker_names = ['SBER', 'GAZP', 'LKOH', 'VTBR', 'ROSN', 'MGNT', 'AFLT', 'ALRS', 'SNGS', 'YNDX', 'TATN', 'NLMK', 'HYDR', 'MOEX', 'FIVE', 'GMKN', 'MAGN']\n",
    "short_ticker_names = ['sr', 'gz', 'lk', 'vb', 'rn', 'mn', 'af', 'al', 'sn', 'yn', 'tt', 'nm', 'hy', 'me', 'fv', 'gk', 'mg', 'ml']\n",
    "\n",
    "ticker_dict = dict(zip(ticker_names, short_ticker_names))\n",
    "\n",
    "with ThreadPoolExecutor() as executor: \n",
    "    executor.map(process_ticker, ticker_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bbca5-4239-40bd-a01a-97b44e01bab7",
   "metadata": {},
   "source": [
    "Удаление повторяющихся данных (в силу того, как МосБиржа возвращала данные (в выходные дни вместо пустого запроса она возвращает последнюю цену за последний рабочий день, то есть 14*20 одинаковых записей, в силу чего эта ячейка позволяла сэкономить, примерно, 300 МБ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc0098-74c0-4bbc-8a36-bf1ef9faf733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(ticker_names):\n",
    "    # Путь к папке с файлами\n",
    "    folder_path = Path('.')\n",
    "    \n",
    "    # Получаем список файлов в папке\n",
    "    files = [f for f in folder_path.iterdir() if f.is_file()]\n",
    "    \n",
    "    for file_name in files:\n",
    "        # Проверяем, что файл имеет нужное расширение и соответствует шаблону\n",
    "        if file_name.suffix == '.csv' and any(ticker in file_name.stem for ticker in ticker_names):\n",
    "            # Извлекаем имя тикера из имени файла\n",
    "            for ticker, short_ticker in zip(ticker_names, ['sr', 'gz', 'lk', 'vb', 'rn', 'mn', 'af', 'al', 'sn', 'yn', 'tt', 'nm', 'hy', 'me', 'fv', 'gk', 'mg', 'ml']):\n",
    "                if ticker in file_name.stem:\n",
    "                    # Формируем новое имя файла\n",
    "                    new_file_name = file_name.stem.replace(f\"{ticker}_full_date_price\", f\"{short_ticker}_full_date_price\") + file_name.suffix\n",
    "                    # Составляем полные пути к старому и новому файлу\n",
    "                    old_file_path = folder_path / file_name\n",
    "                    new_file_path = folder_path / new_file_name\n",
    "                    \n",
    "                    # Переименовываем файл\n",
    "                    old_file_path.rename(new_file_path)\n",
    "                    \n",
    "                    print(f'Файл {file_name} переименован в {new_file_name}')\n",
    "                    break\n",
    "\n",
    "# Пример использования\n",
    "rename_files(['SBER', 'GAZP', 'LKOH', 'VTBR', 'ROSN', 'MGNT', 'AFLT', 'ALRS', 'SNGS', 'YNDX', 'TATN', 'NLMK', 'HYDR', 'MOEX', 'FIVE', 'GMKN', 'MAGN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8fc23-3c8d-409b-b49b-239032dca593",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_names=['sr', 'gz', 'lk', 'vb', 'rn', 'mn', 'af', 'al', 'sn', 'yn', 'tt', 'nm', 'hy', 'me', 'fv', 'gk', 'mg']\n",
    "\n",
    "for ticker_name in ticker_names:\n",
    "    your_file = f\"{ticker_name}_full_date_price.csv\" \n",
    "# Загрузка данных\n",
    "    df = pd.read_csv(your_file)\n",
    "\n",
    "# Удаление повторяющихся строк\n",
    "    unique_rows = df.drop_duplicates()\n",
    "\n",
    "# Сохранение результата\n",
    "    unique_rows.to_csv(your_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11151e32-8d0c-45cb-99bf-a203e5e5f0d9",
   "metadata": {},
   "source": [
    "Итоговые данные не идеальны, поэтому мы их будем обрабатывать в других данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
